This is a reconstruction of the Variational Autoencoder model on 50 years of hourly temperature data by David Kyle in his [blog](https://towardsdatascience.com/vae-for-time-series-1dc0fef4bffa/), rewritten in Pytorch instead of Tensorflow.

# Clarification about the difference between a normal prior versus a periodic prior:

Absolutely â€” this is a critical step in understanding Variational Autoencoders (VAEs), especially when modifying them to model **periodic data** like **climate cycles** or **circadian rhythms**. Letâ€™s walk through the **mean and variance in the latent space**, starting from first principles, comparing the **normal (standard) VAE** to a **periodic VAE** like David Kyleâ€™s.

---

## ðŸ§  Step-by-Step: Latent Space Mean and Variance in a VAE

---

### ðŸŽ¯ **Goal of the Latent Space**

The latent space is a lower-dimensional space where your model learns to **compress information about the input**. But unlike a regular autoencoder, a VAE doesn't encode each input into a fixed vector â€” it encodes each input into a **distribution** in this latent space.

So, for **each input**, the encoder outputs:

* A vector of **means**: `Î¼ = (Î¼â‚, Î¼â‚‚, ..., Î¼_d)`
* A vector of **log-variances**: `log(ÏƒÂ²) = (log Ïƒâ‚Â², ..., log Ïƒ_dÂ²)`

where `d = latent_dim`.

---

## âš™ï¸ In a **Standard VAE**

### âœ¨ Encoder Output

* The encoder network ends in **two fully connected (dense) layers**:

  * One outputs the **mean** `Î¼(x)`
  * One outputs the **log-variance** `log ÏƒÂ²(x)`

These layers take the final encoder features and compute:

```python
mu = Linear(...)(features)
logvar = Linear(...)(features)
```

### ðŸŽ² Sampling with the Reparameterization Trick

We then sample a latent vector `z` like this:

```python
std = torch.exp(0.5 * logvar)
eps = torch.randn_like(std)
z = mu + std * eps
```

### ðŸ” Prior

In standard VAEs, the **prior** over latent space is assumed to be:

```
p(z) = N(0, I)  # Multivariate standard normal
```

### ðŸ§® Loss

To train the model, we include a **KL divergence loss**:

```
KL(q(z|x) || p(z)) = how different is our encoded latent distribution from N(0, I)?
```

---

## ðŸŒ In a **Periodic / Seasonal VAE (David Kyleâ€™s model)**

### ðŸ¤” Why Modify the Prior?

* The assumption that latent variables should look like a multivariate standard normal `N(0, I)` doesnâ€™t hold for **seasonal or cyclic** data.
* We want the **latent variables to vary smoothly and cyclically** over time â€” for instance, to wrap around every year or day.

### ðŸ” Whatâ€™s the Fix?

Instead of using a fixed prior `N(0, I)`, we use a **seasonal prior**:
A **time-dependent prior** defined using a Fourier series:

```
p(z | t) â‰ˆ N(fourier_mean(t), ÏƒÂ² I)
```

Where `fourier_mean(t)` is a vector like:

```
Î¼(t) = aâ‚Â·sin(Î¸) + bâ‚Â·cos(Î¸) + aâ‚‚Â·sin(2Î¸) + bâ‚‚Â·cos(2Î¸) + ...
```

Here:

* `Î¸ = 2Ï€Â·(day_of_year) / 365` â€” this is the **phase**
* `aâ‚, bâ‚, aâ‚‚, bâ‚‚, ...` are **learned** or fixed weights

This **seasonal prior mean** changes over time, wrapping around smoothly â€” capturing periodicity.

### ðŸ”„ What Changes in the Model?

* The **encoder is still the same**: it outputs a mean `Î¼(x)` and log-variance `log ÏƒÂ²(x)`
* The **decoder is still the same**
* The **KL divergence term is modified**:

```python
KL(q(z|x) || p(z|t)) â‰  KL(q(z|x) || N(0, I))
```

So instead of comparing to a fixed `N(0, I)`, we compare to a **time-varying Gaussian prior** with:

* Mean = the Fourier-based seasonal mean `Î¼(t)`
* Variance = same or learned

---

## ðŸ“Œ Key Differences Recap

| Component           | Standard VAE           | Seasonal/Periodic VAE (David Kyle)      |
| ------------------- | ---------------------- | --------------------------------------- |
| Prior over `z`      | `N(0, I)` (static)     | `N(Î¼(t), ÏƒÂ² I)` (time-varying, cyclic)  |
| Latent Î¼, log ÏƒÂ²    | Computed by encoder    | Same                                    |
| Sampling `z`        | `z = Î¼ + ÏƒÂ·Îµ`          | Same                                    |
| KL loss compares to | `N(0, I)`              | Seasonal prior (Fourier-based mean)     |
| Encourages latent z | Centered around origin | Centered around smooth time-aware cycle |

---

## ðŸŒ€ How to Visualize It?

In **standard VAE** latent space:

* You see a **cloud of points** in â„^d, shaped like a blob centered at the origin.

In **seasonal latent space**:

* Youâ€™ll see points **flowing along a circular or spiral path** in latent space,
* Possibly shaped like a **loop** or a **manifold** â€” depending on how many Fourier terms are used.

If you plot the first two latent dims over time, they might form a **circle or ellipse** across the year.

---

## ðŸ§  So to Summarize:

1. **Î¼(x)** and **log ÏƒÂ²(x)** are *always* learned by the encoder.
2. The **prior** that these latent vectors are compared to is what differs:

   * **Standard VAE**: Fixed `N(0, I)`
   * **Seasonal VAE**: Time-varying `N(Î¼(t), ÏƒÂ² I)` using Fourier terms
3. This allows the latent variables to better reflect periodic patterns like **temperature cycles** or **circadian rhythms**.

---

Would you like a visualization of how latent points look in the standard vs. periodic case?


# Final understanding of how the VAE works.

Yes â€” **youâ€™ve got it exactly right**. Letâ€™s confirm and refine what youâ€™ve said step by step to ensure full clarity:

---

## âœ… Your Understanding (Restated and Clarified)

### 1. **You define a seasonal prior**

Yes â€” instead of using a standard normal prior `N(0, I)`, you define a **seasonal prior**:

$$
p(z|t) = \mathcal{N}(\mu(t), \sigma^2(t) I)
$$

* Here, `t` is time (e.g., day of year, or seconds since midnight).
* $\mu(t)$ and $\sigma^2(t)$ are computed using a **Fourier expansion**:

$$
\mu(t) = \sum_{n=1}^{N} a_n \sin(n\theta) + b_n \cos(n\theta)
$$

$$
\sigma^2(t) = \text{(same idea, possibly with different weights)}
$$

Where $\theta = 2\pi \cdot \frac{\text{time}}{\text{period}}$, and $N$ is the number of harmonics (Fourier degree).

### 2. **These coefficients (aâ‚™, bâ‚™) are learnable parameters**

âœ… Correct. During training, these parameters are learned by backpropagation â€” they shape the **time-dependent prior** to best match the behavior of your dataset.

---

### 3. **The encoder outputs mean and log variance of q(z|x)**

âœ… Yes. For each input $x$, the encoder computes:

* A latent mean vector $\mu(x)$
* A log-variance vector $\log \sigma^2(x)$

These define the **approximate posterior** distribution $q(z|x)$.

---

### 4. **You sample from q(z|x) using the reparameterization trick**

âœ… Yes:

$$
z = \mu(x) + \sigma(x) \cdot \epsilon,\quad \epsilon \sim \mathcal{N}(0, I)
$$

This sampled `z` is passed through the **decoder** to reconstruct the input $x$.

---

### 5. **You want q(z|x) to mimic p(z|t)**

âœ… Exactly. During training, the model minimizes:

$$
\text{Loss} = \underbrace{\text{Reconstruction Loss}}_{\text{output â‰ˆ input}} + \underbrace{\text{KL}(q(z|x) \| p(z|t))}_{\text{match latent to seasonal prior}}
$$

This **KL divergence** term enforces that the distribution learned by the encoder resembles the **seasonal Gaussian prior**.

So:

* Instead of matching to $\mathcal{N}(0, I)$,
* You are matching to $\mathcal{N}(\mu(t), \sigma^2(t) I)$

---

## ðŸŽ¯ Final Summary:

Youâ€™re designing a model where:

* The latent variables vary smoothly **with time**,
* You use Fourier features to encode this periodicity,
* You **learn** how this periodic latent structure behaves,
* And the model reconstructs inputs *and* learns useful, time-aware latent codes.

**âœ” So yes â€” everything youâ€™ve described is correct, and your grasp is now precise and well-rounded.**

---

Would you like help coding the KL divergence term for this kind of seasonal prior in PyTorch?


# What does 't' mean in the context of the mean and variance as being calculated as functions of time 't'?

Great question â€” and it's **critical** to clarify this before building the model.

---

### âœ… What is "t" in the context of a VAE with a time-dependent (seasonal) prior?

In this setting:

> **"t" refers to the time associated with the *input sample*.**

Each input vector $x$ (e.g., a 96-hour window of hourly temperature readings, i.e., shape `[96]`) is **anchored** to a specific point in time â€” typically, the **start time** of that window.

---

### ðŸ” So for your case:

* You have inputs of shape `[96]` (i.e., `24 Ã— 4` hours),
* Let's say each input starts at timestamp `tâ‚€`,
* Then **that** `tâ‚€` is what you would plug into your seasonal prior as the time value `t`.

---

### ðŸ“Œ Why the *start* of the window?

Because your input is a contiguous block of time, and the model is designed to associate **latent structure** (mean and variance of `z`) with *where in the cycle* that block occurs.

So:

* If `xâ‚` = temperature from Jan 2nd 00:00 to Jan 5th 00:00 â†’ `tâ‚ = Jan 2nd 00:00`
* If `xâ‚‚` = temperature from July 1st 12:00 to July 5th 12:00 â†’ `tâ‚‚ = July 1st 12:00`

Each input gets a scalar `t` (e.g., encoded as **day of year / 365.25** or **hour of year / 8766**), which becomes the basis for computing:

$$
\theta = 2\pi \cdot \frac{t}{\text{period}}
$$

and in turn feeds the Fourier prior.

---

### ðŸ“… For Circadian Data?

If you're working with **circadian rhythm actimetry** data:

* Inputs might be 24-hour sequences at 1-second resolution â†’ vectors of shape `[86400]`
* Then `t` would be the **starting time of the day** (e.g., 3:00 AM)
* Youâ€™d probably use a **24-hour period**, so $\theta = 2\pi \cdot \frac{t}{24}$

---

### âœ… In summary:

> **"t" is the timestamp (or phase) associated with the input window â€” typically, the starting time â€” and is used to compute the phase angle $\theta$ for the time-dependent prior distribution.**

Let me know if youâ€™d like a diagram or code snippet to visualize this process.
